{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from model import TweetsDataset, TweetClassifer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "openai_embs = pd.read_parquet('./embeddings/ALL_EMBEDDINGS.parquet')\n",
    "openai_embs = openai_embs.set_index('code')\n",
    "\n",
    "\n",
    "\n",
    "def merge_pro_anti(df, pro_col, anti_col):\n",
    "    pro = df[pro_col].values.tolist()\n",
    "    anti = df[anti_col].values.tolist()\n",
    "    merged = []\n",
    "    for i in range(len(pro)):\n",
    "        if pro[i] == 1 and anti[i] == 0:\n",
    "            merged.append(1)\n",
    "        elif pro[i] == 0 and anti[i] == 1:\n",
    "            merged.append(0)\n",
    "        elif pro[i] == 0 and anti[i] == 0:\n",
    "            merged.append(2)\n",
    "        else:\n",
    "            print(f' row {i} has both pro and anti')\n",
    "            merged.append(2)\n",
    "    return merged\n",
    "\n",
    "def score_model(model, dataloader, multi_calss = 'raise'):\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x, logits=True)\n",
    "            ground_truth.append(y)\n",
    "            predictions.append(y_pred)\n",
    "    ground_truth = torch.concat(ground_truth).detach().cpu()\n",
    "    predictions = torch.concat(predictions).detach().cpu()\n",
    "    rocauc = roc_auc_score(ground_truth, predictions, multi_class=multi_calss)\n",
    "    predictions = torch.where(predictions > 0.5, 1, 0).type(torch.float32)\n",
    "    if y_pred.shape[1]>1:\n",
    "        mcm =  multilabel_confusion_matrix(ground_truth, predictions)\n",
    "        true_negatives = mcm[:,0,0]\n",
    "        false_negatives = mcm[:,1,0]\n",
    "        false_positives = mcm[:,0,1]\n",
    "        true_positives = mcm[:,1,1]\n",
    "    else:\n",
    "        cm = confusion_matrix(ground_truth, predictions)\n",
    "        true_negatives  = cm[0,0]\n",
    "        false_negatives = cm[1,0]\n",
    "        false_positives = cm[0,1]\n",
    "        true_positives  = cm[1,1]\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    scores = {\n",
    "        'accuracy': (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives),\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': 2 * (precision * recall) / (precision + recall),\n",
    "        'rocauc': rocauc\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def get_tweets_embeddings(tweets, codes):\n",
    "    tweets_embeddings = []\n",
    "    for i in range(len(codes)):\n",
    "        code = codes[i]\n",
    "        row = openai_embs.loc[code]\n",
    "        tweets_embeddings.append(row['embedding'])\n",
    "    tweets_embeddings = torch.tensor(tweets_embeddings, device=device, dtype=torch.float32)\n",
    "    return tweets_embeddings\n",
    "\n",
    "def calculate_pos_weights(data):\n",
    "    class_counts = data.sum(axis=0).to_numpy()\n",
    "    pos_weights = np.ones_like(class_counts)\n",
    "    neg_counts = [len(data)-pos_count for pos_count in class_counts]\n",
    "    for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n",
    "        pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n",
    "    return torch.as_tensor(pos_weights, dtype=torch.float, device=device)\n",
    "\n",
    "def read_data(all=False):\n",
    "    if all:\n",
    "        df = pd.read_csv('./data/combined_reports.csv')\n",
    "        permalinks = df['permalink'].values\n",
    "        codes = [permalink.split('/')[-1] for permalink in permalinks]\n",
    "        df['code'] = codes\n",
    "        return df\n",
    "    df = pd.read_excel('./data/data.xlsx')\n",
    "    # Change weired values\n",
    "    df.at[596, 'Not about Sudan'] = 0\n",
    "    df.at[680, 'pro RSF'] = 0\n",
    "    df.at[774, 'Likely bot'] = 0\n",
    "    df.at[774, 'Likely not a bot'] = 0\n",
    "    df.at[687, 'anti SAF'] = 0\n",
    "    permalinks = df['permalink'].values\n",
    "    codes = [permalink.split('/')[-1] for permalink in permalinks]\n",
    "    df['code'] = codes\n",
    "    return df\n",
    "\n",
    "def get_xy(df, tweets_col = 'post', labels_cols=None):\n",
    "    X = df[tweets_col].reset_index(drop=True)\n",
    "    if labels_cols:\n",
    "        Y = df[labels_cols].reset_index(drop=True)\n",
    "    else:\n",
    "        Y = df.drop(columns=['post']).reset_index(drop=True)\n",
    "    codes = df['code'].reset_index(drop=True)\n",
    "    return X, Y, codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(867, 39)\n"
     ]
    }
   ],
   "source": [
    "df = read_data().dropna().reset_index(drop=True)\n",
    "test_size = 0.3\n",
    "labels = ['pro RSF', 'anti RSF', 'anti SAF', 'pro SAF', 'Pro peace,', 'anti peace', 'Pro War',\n",
    "       'anti war', 'pro civilian', 'anti civilians', 'no polarisation', 'Geopolticis', 'Sudanese', 'Not Sudanese']\n",
    "random_state = 42\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RSF'] = merge_pro_anti(df, 'pro RSF', 'anti RSF')\n",
    "df['SAF'] = merge_pro_anti(df, 'pro SAF', 'anti SAF')\n",
    "df['peace'] = merge_pro_anti(df, 'Pro peace,', 'anti peace')\n",
    "df['war'] = merge_pro_anti(df, 'Pro War', 'anti war')\n",
    "df['civilians'] = merge_pro_anti(df, 'pro civilian', 'anti civilians')\n",
    "labels = ['RSF', 'SAF', 'peace', 'war', 'civilians', 'no polarisation', 'Geopolticis', 'Sudanese', 'Not Sudanese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 606, Val: 182, Test: 79\n"
     ]
    }
   ],
   "source": [
    "X, Y, codes = get_xy(df, labels_cols=labels)\n",
    "X_train, X_test, Y_train, Y_test, codes_train, codes_test = train_test_split(X, Y, codes, test_size=test_size, random_state=random_state)\n",
    "X_val, X_test, Y_val, Y_test, codes_val, codes_test = train_test_split(X_test, Y_test, codes_test, test_size=test_size, random_state=random_state)\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Classes = 9\n",
      "Emb Dim = 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/kdv7hqy122dch3mdx3jbs0b40000gp/T/ipykernel_28848/2814180583.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  tweets_embeddings = torch.tensor(tweets_embeddings, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_tweets_embeddings(X_train.to_list(), codes=codes_train.to_list())\n",
    "val_embeddings   = get_tweets_embeddings(X_val.to_list()  , codes=codes_val.to_list()  )\n",
    "test_embeddings  = get_tweets_embeddings(X_test.to_list() , codes=codes_test.to_list() )\n",
    "train_labels = torch.tensor(Y_train.to_numpy(), dtype=torch.float32)\n",
    "val_labels = torch.tensor(Y_val.to_numpy(), dtype=torch.float32)\n",
    "test_labels = torch.tensor(Y_test.to_numpy(), dtype=torch.float32)\n",
    "num_classes = len(labels)\n",
    "embeddings_dim = train_embeddings.shape[1]\n",
    "print(f'Num Classes = {num_classes}')\n",
    "print(f'Emb Dim = {embeddings_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def multi_class_weights(data, label):\n",
    "    y = data[label].values\n",
    "    w = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "    w =torch.tensor(w, dtype=torch.float32).to(device=device)\n",
    "    return(w)\n",
    "\n",
    "def score_model(model, dataloader, multi_calss = 'raise'):\n",
    "    model.eval()\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            ground_truth.append(y)\n",
    "            predictions.append(y_pred)\n",
    "    ground_truth = torch.concat(ground_truth).detach().cpu()\n",
    "    predictions = torch.concat(predictions).detach().cpu()\n",
    "    gt = [yp.item() for yp in ground_truth]\n",
    "    preds = [yp.argmax().item() for yp in predictions]\n",
    "    acc = accuracy_score(gt, preds)\n",
    "    if multi_calss == 'raise':\n",
    "        predictions = torch.tensor([yp.argmax().item() for yp in predictions])\n",
    "    rocauc = roc_auc_score(ground_truth, predictions, multi_class=multi_calss)\n",
    "    predictions = torch.where(predictions > 0.5, 1, 0).type(torch.float32)\n",
    "    scores = {\n",
    "        'rocauc': rocauc,\n",
    "        'accuracy': acc\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, num_epochs, optimizer, loss_fn, model_name):\n",
    "    logs = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_rocauc':[],\n",
    "        'val_rocauc':[]\n",
    "    }\n",
    "    val_rocauc_max = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x, logits=True)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch%50 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for x, y in val_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_pred = model(x)\n",
    "                    val_loss += loss_fn(y_pred, y)\n",
    "            if y_pred.shape [1] == 2:\n",
    "                multi_class = 'raise'\n",
    "            else:\n",
    "                multi_class = 'ovo'\n",
    "            train_rocauc = score_model(model, train_dataloader, multi_calss=multi_class)['rocauc']\n",
    "            val_rocauc = score_model(model, val_dataloader, multi_calss=multi_class)['rocauc']\n",
    "            train_acc = score_model(model, train_dataloader, multi_calss=multi_class)['accuracy']\n",
    "            val_acc = score_model(model, val_dataloader, multi_calss=multi_class)['accuracy']\n",
    "            if val_rocauc > val_rocauc_max:\n",
    "                val_rocauc_max = val_rocauc\n",
    "                torch.save(model.state_dict(), f'./models/best_{model_name}.pth')\n",
    "            print(f'Epoch: {epoch} -- Train Loss: {loss.item() :.4f} RocAuc = {train_rocauc*100 :.4f} Acc = {train_acc*100 :.4f}|| Val Loss: {val_loss.item() :.4f} RocAuc = {val_rocauc*100 :.4f} Acc = {val_acc*100 :.4f}')\n",
    "            logs['train_loss'].append(loss.item())\n",
    "            logs['val_loss'].append(val_loss.item())\n",
    "            logs['train_rocauc'].append(train_rocauc)\n",
    "            logs['val_rocauc'].append(val_rocauc)\n",
    "        torch.save(model.state_dict(), f'./models/lattest_{model_name}.pth')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,label in enumerate(labels):\n",
    "    print(i,label, 'WITH CLASS WEIHGTS')\n",
    "    num_epochs = 300\n",
    "    batch_size =512\n",
    "    learning_rate = 1e-4\n",
    "    hidden_dim = 2048\n",
    "    train_ds = TweetsDataset(train_embeddings, train_labels[:,i])\n",
    "    val_ds = TweetsDataset(val_embeddings, val_labels[:,i])\n",
    "    test_ds = TweetsDataset(test_embeddings, test_labels[:,i])\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    num_classes = len(train_labels[:,i].unique())\n",
    "    model = TweetClassiferMultiClass(embeddings_dim, num_classes, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=multi_class_weights(Y, label))\n",
    "    model_name = f'multiclass_singlelabel_{label}'\n",
    "    model = train(model, train_dataloader, val_dataloader, num_epochs, optimizer, loss_fn, model_name)\n",
    "    print('==========================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_class(dl, ds, labels, embeddings_dim, hidden_dim, device):\n",
    "    all_scores = {}\n",
    "    for i,label in enumerate(labels):\n",
    "        num_classes = len(ds[:,i].unique())\n",
    "        print(label)\n",
    "        model_name = f'multiclass_singlelabel_{label}'\n",
    "        model = TweetClassiferMultiClass(embeddings_dim, num_classes, hidden_dim=hidden_dim).to(device)\n",
    "        model.load_state_dict(torch.load(f'./models/best_{model_name}.pth'))\n",
    "        model.eval()\n",
    "        if num_classes == 2:\n",
    "            multi_class = 'raise'\n",
    "        else:\n",
    "            multi_class = 'ovo'\n",
    "        scores = score_model(model, dl, multi_calss=multi_class)\n",
    "        all_scores[label] = scores\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "test_ds = TweetsDataset(test_embeddings, test_labels)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "scores = predict_multi_class(test_dataloader, labels, labels, embeddings_dim, hidden_dim, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
